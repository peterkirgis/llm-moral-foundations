# !pip install edsl > /dev/null 2>&1

import pandas as pd
import numpy as np
import os
from google.colab import userdata
os.environ['EXPECTED_PARROT_API_KEY'] = userdata.get('EXPECTED_PARROT_API_KEY')
from edsl import Model, ModelList, QuestionLinearScale, QuestionMultipleChoice, Survey, Scenario, ScenarioList, Agent, AgentList, Notebook, Coop

# import data
data = pd.read_csv('data/survey/vignettes_short.csv')
# convert first column to a list
vignettes = data.iloc[:,0].tolist()
# randomize order of vignettes
np.random.shuffle(vignettes)

def run_survey_for_model(model_name, service, vignettes, data, moral_judgment_survey_result, samples):
    """
    This function assumes that:
      - 'vignettes' is a list of dilemmas.
      - 'data' is the main DataFrame with a "Scenario" column.
      - 'moral_judgment_survey_results' will be generated by running the survey for the given model.
         (Here, we show the survey-creation code inline.)
    """
    # Select the model.
    if model_name != "o3-mini" and model_name != "gpt-4.5-preview" and model_name != "deepseek-reasoner":
      model = Model(model_name, service_name=service, logprobs=True)
    else:
      model = Model(model_name, service_name=service)

    # Create a list of scenarios.
    scenarios = ScenarioList.from_list("dilemma", vignettes)

    # Create the question format.
    q = QuestionLinearScale(
      question_name = "moral_judgment",
      question_text = "Please rate how morally wrong this behavior is on the following five-point scale: {{dilemma}}",
      question_options = [0,1,2,3,4],
      option_labels = {0: "Not at all wrong", 1: "Not too wrong", 2: "Somewhat wrong", 3: "Very wrong", 4: "Extremely wrong"}
    )

    # Loop through the scenarios.
    questions = q.loop(scenarios)

    # Create the questionnaire.
    moral_judgment_survey = Survey(questions=questions)

    # Run the survey.
    moral_judgment_survey_results = moral_judgment_survey.by(model).run(n=samples)

    # Process the results.
    results = []

    # Loop over number of agents
    # for n in range(len(g20_countries)):
    # Loop over each iteration (assumed 0 to len(data)-1)
    for i in range(len(data)):
        # Extract the question text from the Spark DataFrame.
        qtext_row = moral_judgment_survey_results.select(f"question_text.moral_judgment_{i}_question_text").to_list()
        prefix = "Please rate how morally wrong this behavior is on the following five-point scale: "
        question_list = [q.replace(prefix, "") for q in qtext_row]
        qtext = question_list[0] if question_list is not None else None

        # Extract the survey response.
        response = moral_judgment_survey_results.select(f"answer.moral_judgment_{i}").to_list()
        response = [x if x is not None else np.nan for x in response]
        comment = moral_judgment_survey_results.select(f"comment.moral_judgment_{i}_comment").to_list()
        raw_output = moral_judgment_survey_results.select(f"raw_model_response.moral_judgment_{i}_raw_model_response").to_list()
        mean_response = np.nanmean(response)

        results.append({
            "Scenario": qtext,
            "Answer": mean_response if response is not None else None,
            "Comment": comment[0] if comment is not None else None,
            "Raw Output": raw_output[0] if raw_output is not None else None

        })

    # Create a DataFrame from the results.
    results_df = pd.DataFrame(results)
    # Append the service and model info.
    results_df["Service"] = service
    results_df["Model"] = model_name
    return results_df, moral_judgment_survey_results

# Our dictionary of models.
sampled_model_dict = {
    "openai" : ["gpt-4.5-preview", "o3-mini"],
    "google" : ["gemini-2.0-flash", "gemini-2.5-flash-preview-04-17", "gemini-1.5-pro", "gemini-2.5-pro-preview-03-25"],
    "anthropic" : ["claude-3-5-haiku-20241022", "claude-3-5-sonnet-20241022", "claude-3-7-sonnet-20250219" "claude-3-opus-20240229"],
    "deep_infra" : ["meta-llama/Meta-Llama-3.1-70B-Instruct", "meta-llama/Meta-Llama-3.1-405B-Instruct", "meta-llama/Llama-4-Scout-17B-16E-Instruct"],
    "deepseek" : ["deepseek-chat", "deepseek-reasoner"]
}

logprobs_model_dict = {
   "openai" : ["gpt-3.5-turbo", "gpt-4-turbo", "gpt-4o", "gpt-4.1"],
   "grok" : ["grok-2-1212", "grok-3-beta"]
}

# List to store results for all models.
sampled_results_list = []

# For each service and model in our dictionary, run the survey and extract results.
for service, models in sampled_model_dict.items():
    for model_name in models:
        print(f"Running survey for model: {model_name} ({service})")
        # Run the survey for this model.
        results_df, model_response = run_survey_for_model(model_name, service, vignettes, data, moral_judgment_survey_results=None, samples=10)
        # Note: The above function creates its own moral_judgment_survey_results.
        sampled_results_list.append(results_df)

# Concatenate the individual DataFrames into one.
sampled_final_df = pd.concat(sampled_results_list, ignore_index=True)

# List to store results for all models.
logprobs_results_list = []

# For each service and model in our dictionary, run the survey and extract results.
for service, models in logprobs_model_dict.items():
    for model_name in models:
        print(f"Running survey for model: {model_name} ({service})")
        # Run the survey for this model.
        results_df, model_response = run_survey_for_model(model_name, service, vignettes, data, moral_judgment_survey_results=None, samples=1)
        # Note: The above function creates its own moral_judgment_survey_results.
        logprobs_results_list.append(results_df)

# Concatenate the individual DataFrames into one.
logprobs_final_df = pd.concat(logprobs_results_list, ignore_index=True)

# Send output to csv
sampled_final_df.to_csv("sampled_model_responses.csv")
logprobs_final_df.to_csv("logprobs_model_responses.csv")
